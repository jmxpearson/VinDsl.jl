{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Toward a domain-specific language for variational inference\n",
    "\n",
    "John Pearson   \n",
    "DukeML group meeting   \n",
    "2-18-16 \n",
    "\n",
    "<img src=\"http://pearsonlab.github.io/images/plab_logo_dark.svg\" width=\"300\", align=\"left\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What's variational inference?\n",
    "\n",
    "Generative model for data: $p(y|\\theta)/Z$  \n",
    "Approximate model posterior $q(\\theta)$\n",
    "\n",
    "Maximize **E**vidence **L**ower **Bo**und (ELBO) wrt $\\theta$:\n",
    "\n",
    "$$\n",
    "\\log Z \\ge -KL\\left(q \\middle\\| p\\right) = \\mathcal{L} = \\mathbb{E}_q[\\log p(y|\\theta)] + \\mathcal{H}[q]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Why variational inference?\n",
    "\n",
    "- Scales well\n",
    "- Can use well-studied optimization techniques\n",
    "\n",
    "# Drawbacks:\n",
    "- !@$*&# hard to code\n",
    "- Can't quickly spec out a model like with Stan or JAGS/BUGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Why is it difficult?\n",
    "\n",
    "- Traditionally, conjugate models $\\Longrightarrow$ lots of algebra\n",
    "- Gradient descent requires gradient calculation\n",
    "- for non-stochastic models $\\mathcal{L}$ should increase on every iteration, but requires extra calculation of objective &mdash; tricky to get right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Lots of great VI tricks\n",
    "- stochastic variational inference (SVI): [Hoffman et al.](http://dl.acm.org/citation.cfm?id=2502622)\n",
    "- black box variational inference (BBVI): [Ranganath et al.](http://arxiv.org/abs/1401.0118)\n",
    "- control variates: [Paisley et al.](http://arxiv.org/abs/1206.6430)\n",
    "- local expectation gradients (LEG): [Titsias and L&aacute;zaro-Gredilla](http://papers.nips.cc/paper/5678-local-expectation-gradients-for-black-box-variational-inference)\n",
    "- neural variational inference (NVIL): [Mnih and Gregor](http://arxiv.org/abs/1402.0030)\n",
    "\n",
    "# ... but hard to mix and match\n",
    "- Stan does (only) BBVI\n",
    "- but no discrete params\n",
    "- only mean field or full (Gaussian) covariance\n",
    "- custom Stan requires C++ \n",
    "- [VIBES](http://vibes.sourceforge.net/) (is abandonware?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# What's the ideal?\n",
    "- write math, get code &mdash; a domain-specific language (DSL)\n",
    "- easily generalize to different numbers of indices, structures\n",
    "- only weakly opinionated about model structure or inference\n",
    "- model code should be *hackable*\n",
    "    - easy to use prefab pieces\n",
    "    - not hard to write custom vb tricks\n",
    "    - fast prototyping\n",
    "- no (or minimal) algebra\n",
    "    - simple expectations\n",
    "    - automatic gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Introducing...\n",
    "![](http://www.joblo.com/newsimages1/vin.diesel_1920x1200_961)\n",
    "## VinDsl.jl: Fast and furious variational inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What makes VinDsl special?\n",
    "- written in Julia\n",
    "- Sensible model primitives\n",
    "- Automatic index bookkeeping\n",
    "- Expectation calculus\n",
    "- Exploiting conjugacy\n",
    "- Automatic gradients$^*$\n",
    "- Multiple inference strategies$^*$\n",
    "\n",
    "\n",
    "*: Coming Soon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Model structure:\n",
    "### Main idea: Factor graphs\n",
    "- idea from Dahua Lin in [this talk](http://people.csail.mit.edu/dhlin/jubayes/julia_bayes_inference.pdf)\n",
    "<img src=\"http://research.microsoft.com/en-us/um/people/cmbishop/prml/prmlfigs-png/Figure8.51.png\" width=\"200\">\n",
    "- Nodes: arrays of distributions\n",
    "- Factors $\\leftrightarrow$ terms in variational objective\n",
    "    - but not locked in to graphical model structure!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Nodes can be generated from any distribution type in Julia\n",
    "- indices inferred automagically\n",
    "- expectations, entropy, etc. *just work*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "push!(LOAD_PATH, \"/Users/jmxp/code/VinDsl.jl/src\")\n",
    "using VinDsl\n",
    "using Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dims = (20, 6)\n",
    "\n",
    "μ[j] ~ Normal(zeros(dims[2]), ones(dims[2]))\n",
    "τ[j] ~ Gamma(1.1 * ones(dims[2]), ones(dims[2]))\n",
    "μ0[j] ~ Const(zeros(dims[2]))\n",
    "\n",
    "y[i, j] ~ Const(rand(dims));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Nodes: under the hood\n",
    "- nodes define the q/approximate posterior/recognition model\n",
    "- `~` defines a node\n",
    "- can use any distribution defined in the Distributions package\n",
    "- code parses the left and right-hand sides\n",
    "    - indices on left get tracked and assigned to dimensions of parameter arrays\n",
    "    - code is rewritten as a call to a node constructor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Factors\n",
    "- Factors are terms in the generative model\n",
    "- Right now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = @factor LogNormalFactor y μ τ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In future:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@pmodel begin\n",
    "    y ~ Normal(μ, τ)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "New factor types can be defined with yet another macro:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@deffactor LogNormalFactor [x, μ, τ] begin\n",
    "    -(1/2) * ((E(τ) * ( V(x) + V(μ) + (E(x) - E(μ))^2 ) + log(2π) + Elog(τ)))\n",
    "end\n",
    "\n",
    "@deffactor LogGammaCanonFactor [x, α, β] begin\n",
    "    (E(α) - 1) * Elog(x) - E(β) * E(x) + E(α) * E(β) - Eloggamma(α)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Uses a \"mini-language\" with `E(x)` $\\equiv \\mathbb{E}[X]$, `V(x)` $\\equiv \\textrm{cov}[X]$, etc.  \n",
    "- Again, no need to track indices\n",
    "    - multivariate distributions (Dirichlet, MvNormal) are automatically multivariate in these expressions\n",
    "- `VinDsl` generates a `value(f)` function that handles indices appropriately and sums over the dimensions of the array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Models are just factor graphs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dims = (20, 6)\n",
    "\n",
    "# note: it won't matter much how we initialize here\n",
    "μ[j] ~ Normal(zeros(dims[2]), ones(dims[2]))\n",
    "τ[j] ~ Gamma(1.1 * ones(dims[2]), ones(dims[2]))\n",
    "μ0[j] ~ Const(zeros(dims[2]))\n",
    "τ0[j] ~ Const(2 * ones(dims[2]))\n",
    "a0[j] ~ Const(1.1 * ones(dims[2]))\n",
    "b0[j] ~ Const(ones(dims[2]))\n",
    "\n",
    "y[i, j] ~ Const(rand(dims))\n",
    "\n",
    "# make factors\n",
    "obs = @factor LogNormalFactor y μ τ\n",
    "μ_prior = @factor LogNormalFactor μ μ0 τ0\n",
    "τ_prior = @factor LogGammaCanonFactor τ a0 b0\n",
    "\n",
    "m = VBModel([μ, τ, μ0, τ0, a0, b0, y], [obs, μ_prior, τ_prior]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Models have a separate update strategy for each node\n",
    "- allows mix-and-match inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Index Bookkeeping\n",
    "- nodes have associated indices\n",
    "- factors know which indices go with which nodes, which indices to sum over\n",
    "    - inner indices belong to, e.g., elements of a multivariate normal (should not be separated)\n",
    "    - outer indices correspond to replicates of \"atomic\" variables  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So this is easy: `i` is inner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = 5\n",
    "μ[i] ~ MvNormalCanon(zeros(d), diagm(ones(d)))\n",
    "Λ[i, i] ~ Wishart(float(d), diagm(ones(d)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But here, `i` is inner for $\\mu$ but not for $\\tau$. In any factor combining these two, $\\tau$ will be treated like a vector because it matches an inner index for some node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "μ[i] ~ MvNormalCanon(zeros(d), diagm(ones(d)))\n",
    "τ[i] ~ Gamma(1.1 * ones(d), ones(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Expression Nodes\n",
    "\n",
    "- We want to define nodes that combine nodes (`ExprNode`s)\n",
    "- But we also want `E(x)` to work for these cases\n",
    "- `ExprNodes` are like a cross between `Factor`s and `Node`s\n",
    "    - represent variables in the model, not ELBO terms\n",
    "    - but need to track multiple indices like factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Solution: expectation calculus\n",
    "- because Julia allows us to parse Julia code natively, we can rewrite expressions\n",
    "- define macros that \"wrap\" `E`, etc. using linearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0612025838935821"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x ~ Normal(rand(), rand())\n",
    "y ~ Normal(rand(), rand())\n",
    "\n",
    "@expandE E(x.data[1] + y.data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":(E(x) + E(y) * E(z) + 5)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macroexpand(:(@expandE E(x + y * z + 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: assumes all nodes are independent!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Future\n",
    "- implement rules for `V`, etc.\n",
    "- allow Julia expressions like `sum` and `product` to work over selected indices\n",
    "- Eventually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@pmodel begin\n",
    "    x[i, k] ~ ...\n",
    "    y[j, k] ~ ...\n",
    "    \n",
    "    z := sum(x, i) + sum(y, j)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conjugacy\n",
    "- right now `VinDsl` goes out of its way to handle conjugacy between nodes\n",
    "- conjugate relationships not automatically detected, but easy to define\n",
    "- `@defnaturals` returns expected sufficient statistics from a factor for a given target distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@defnaturals LogNormalFactor μ Normal begin\n",
    "    Ex, Eτ = E(x), E(τ)\n",
    "    (Ex * Eτ, -Eτ/2)\n",
    "end\n",
    "\n",
    "@defnaturals LogNormalFactor τ Gamma begin\n",
    "    v = V(x) + V(μ) + (E(x) - E(μ))^2\n",
    "    (1/2, v/2)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Automatic gradients\n",
    "- we want a lot more than conjugacy-based coordinate ascent\n",
    "- at the very least, be able to perform a brute-force optimization step (coming very soon!)\n",
    "- automatic differentiation is an *exact* (to machine tolerance) way of calculating gradients based on arbitrary code\n",
    "- multiple packages in Julia, but some changes to Distributions needed\n",
    "    - very high on my list\n",
    "- will allow SVI, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Future plans:\n",
    "- two models for my own work\n",
    "    - factorial HMM (gamma and log-normal)\n",
    "    - linear state space\n",
    "- just conjugacy + simple convex opt steps\n",
    "- eventually:\n",
    "    - nicer DSL: `@qmodel` and `@pmodel`, `:=` for `ExprNode`s\n",
    "    - minibatch support, SVI\n",
    "    - LEG, control variates\n",
    "    - Jacobians for all distributions $\\longrightarrow$ BBVI\n",
    "- **Optimize code generation for speed**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# VinDsl needs your help!\n",
    "\n",
    "![](http://cdn.badassdigest.com/uploads/images/30118/the_pacifier02crop__index.jpg)\n",
    "\n",
    "Open sourcing soon:\n",
    "- docs\n",
    "- tests\n",
    "- better ideas!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Julia 0.4.3",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
